# 의사결정나무의 장점과 단점
> 🌳 **의사결정나무 Decision Tree**
* 데이터가 주어졌을 때, **어떤 질문을 어떤 순서로 해야 예측 및 분류를 잘 할 수 있을지 학습**해서 만들어지는 나무 구조의 모델
* 뿌리 노드(root node)에 모든 예측/분류 대상을 포함하고, **특정 분류 기준에 부합하는지의 여부에 따라서 두 개의 가지로 나누어 데이터를 분리시키는 과정을 반복**함
> 👍 **의사결정나무의 장점**
* 범주형, 연속형 수치를 모두 예측할 수 있음(classifier, regressor)
* 모델의 직관성이 높아 결정 이유와 분석 결과를 해석하기 쉬움
* 변수 간의 상호작용과 비선형성을 반영해 예측의 정확도를 높임 = 수학적 가정이 불필요한 비모수적 모형
* 시장조사, 의학연구, 품질관리, 고객 타겟팅 등 다양한 분야와 예측에 유용하게 쓰임
> 👎 **의사결정나무의 단점**
* 과적합 문제가 발생할 수 있음 = 적절한 가지치기(pruning)와 샘플링 필요
* 분류 기준값의 경계선 근처 자료 값에 대해 오차가 클 수 있음
* 각 예측 변수의 효과를 파악하기 어려움
* 새로운 자료에 대한 예측 불안정
# 부스팅 모델의 특징과 종류, 장단점
> ⏫ **부스팅 알고리즘 Boosting**
* 하나의 모델이 과적합되지 않도록, 약한 모델 여러 개를 결합시켜 결과를 종합하는 모델
  * ❗ 배깅과의 차이점
    * 배깅 : 개별 모델 독립 작동 ➡️ 결과를 병렬로 합침
    * 부스팅 : 특정 모델 작동 및 결과 나옴 ➡️ 다른 모델의 입력값으로 사용함
* 다른 모델의 결과를 입력값으로 사용해 단점을 보완하며 학습해나가고, 결과적으로 모델이 점점 강해지게 되는 것에서 boosting이라는 표현이 비롯함
* 노이즈와 이상치에 대해 대체로 견고한 경향을 보임
* 분류, 회귀, 시계열 자료 등 다양한 예측에 사용 가능함
* 대규모 데이터를 쉽게 처리할 수 있는, 확장성이 뛰어난 모델 존재(XGBoost, LightGBM 등)
> 📊 **부스팅 모델의 종류**
|모델명            |설명                            |특징                         |
|----------------|-------------------------------|-----------------------------|
|AdaBoost|모델이 **예측에 실패한 샘플이 다시 학습될 수 있도록 가중치를 부여**해, 다음 모델에서는 더 집중하게 만드는 모델|간단한 방법이지만 이상치에 민감함|
|Gradient Boosting|모델이 **예측하고 남은 잔차를 다음 모델에서 다시 학습하여 예측**함으로써 잔차를 줄이는 모델            |과적합 가능성이 있고 처리 소요 시간이 김|
|XGBoost          |**Gradient Boosting을 개선한 것**으로 병렬 분산 컴퓨팅, 내장된 정규화 기능, 누락된 값 처리 알고리즘 등이 추가된 모델 |다양한 하이퍼파라미터 튜닝 가능, 범주형에 수동 인코딩 필요|
|LightGBM       |**데이터 정렬 방식을 바꿔 학습 속도를 대폭 개선**한 GBM모델|처리 속도가 빠르고 메모리를 효율적으로 사용할 수 있으며 대용량 데이터에 적합함|
|CatBoost        |**학습 데이터의 일부만으로 잔차계산을 하는 기능과 범주형 변수 학습 성능**이 뛰어나도록 개선한 모델|적은 튜닝만으로 성능이 좋아짐, 예측 속도가 매우 빠름|
